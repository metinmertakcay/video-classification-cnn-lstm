{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script is used to extract key frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "from random import randrange\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder if not exist\n",
    "def create_folder(folder):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification property of the classification layer removed.\n",
    "model = models.inception_v3(pretrained=True)\n",
    "model.fc = nn.Identity()\n",
    "\n",
    "is_cpu_available = torch.cuda.is_available()\n",
    "if is_cpu_available:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cluster = 16\n",
    "n_components = 50\n",
    "pca = PCA(n_components)\n",
    "dataset = 'dataset_ucf_video'\n",
    "outputs = 'dataset_ucf_video_keyframe1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 299, 299])\n",
      "torch.Size([1, 3, 299, 299])\n",
      "torch.Size([1, 3, 299, 299])\n",
      "torch.Size([1, 3, 299, 299])\n",
      "torch.Size([1, 3, 299, 299])\n",
      "torch.Size([1, 3, 299, 299])\n",
      "torch.Size([1, 3, 299, 299])\n",
      "torch.Size([1, 3, 299, 299])\n",
      "torch.Size([1, 3, 299, 299])\n",
      "torch.Size([1, 3, 299, 299])\n",
      "torch.Size([1, 3, 299, 299])\n",
      "torch.Size([1, 3, 299, 299])\n",
      "torch.Size([1, 3, 299, 299])\n",
      "torch.Size([1, 3, 299, 299])\n",
      "torch.Size([1, 3, 299, 299])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4a64d6a69102>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                     \u001b[0mfeature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                     \u001b[0mfeature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m                     \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Pytorch expect image = [channels, height, width]\n",
    "cluster_id = np.linspace(0, num_cluster - 1, num_cluster, dtype=np.int16)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    dtypes = os.listdir(dataset)\n",
    "    \n",
    "    for dtype in dtypes:\n",
    "        dtype_path = os.path.join(dataset, dtype)\n",
    "\n",
    "        create_folder(os.path.join(outputs, dtype))\n",
    "\n",
    "        categories = os.listdir(dtype_path)\n",
    "        for category in categories:\n",
    "            category_path = os.path.join(dtype_path, category)\n",
    "\n",
    "            # create category \n",
    "            create_folder(os.path.join(outputs, dtype, category))\n",
    "\n",
    "            video_count = 0\n",
    "            videos = os.listdir(category_path)\n",
    "            for video in videos:\n",
    "                video_path = os.path.join(category_path, video)\n",
    "        \n",
    "                vidcap = cv2.VideoCapture(video_path)\n",
    "                success, img = vidcap.read()\n",
    "\n",
    "                frames = []\n",
    "                features = []\n",
    "                while success:\n",
    "                    frames.append(img)\n",
    "\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    img = cv2.resize(img, (299, 299), interpolation=cv2.INTER_CUBIC)\n",
    "                    img = np.expand_dims(img, axis=0)\n",
    "                    img = np.transpose(img, (0, 3, 1, 2))\n",
    "                    img = torch.from_numpy(img)\n",
    "                    if is_cpu_available:\n",
    "                        img = img.cuda()\n",
    "                    print(img.shape)\n",
    "                    feature = model.forward(img)\n",
    "                    feature = feature.data.cpu().numpy()[0]\n",
    "                    features.append(feature)\n",
    "\n",
    "                    success, img = vidcap.read()\n",
    "\n",
    "                i = 0\n",
    "                while len(features) < n_components: \n",
    "                    features.append(features[i])\n",
    "                    frames.append(frames[i])\n",
    "                    \n",
    "                    i += 1\n",
    "                \n",
    "                features = pca.fit_transform(features)\n",
    "                \n",
    "                kmeans = KMeans(n_clusters=num_cluster, random_state=0).fit(features)\n",
    "                kmeans_labels = list(kmeans.labels_)\n",
    "\n",
    "                indexes = []\n",
    "                for id in cluster_id:\n",
    "                    if id in kmeans_labels:\n",
    "                        index = kmeans_labels.index(id)\n",
    "                        indexes.append(index)\n",
    "                    else:\n",
    "                        index = randrange(len(kmeans_labels))\n",
    "                        indexes.append(index)\n",
    "\n",
    "                indexes.sort()\n",
    "                for index in indexes:\n",
    "                    cv2.imwrite(os.path.join(outputs, dtype, category, '{} - {}.jpg'.format(video_count, index)), frames[index])\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                video_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
